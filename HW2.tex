\documentclass{article}      
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}

\begin{document}

\title{%
  236501: Assignment \#2 \\
  \large Reversi \\
    Fall Semester '17-'18}

\author{
  Avidan, Eyal \\
  \texttt{205796469}
  \and
  Goaz, Or \\
  \texttt{307950113}
}

\maketitle

\section*{A. Simple Player}
\subsection*{1. Simple Player vs. Random Player}
The simple player's heuristic just tries to maximize the number of units the player has over the opponent (unless a goal state / lose state is acheived), which is a \textbf{very simple} method. When running against the \emph{random} player, the simple player one 2 out of 3 matches - both as the starting player and not. \\~\\
This really indicates the fact that the simple heuristic is no a good strategy, as it's prone to lead to foolish moves an expert player would never make, and thus the random player was able to beat it (twice!)

\section*{B. Better Player}
\subsection*{1. Heuristic Explanation}
We will use a heuristic comprised of the following factors
\begin{itemize}
\item \textbf{Coin Parity} - Similar to what the Simple Player attempted, only a negative weight will be set for having less units than the opponent, and instead of looking at the difference we will look at the share of coins owned by the player, e.g.
$$
	sign(my > op) \cdot \frac {my}{my + op}
$$
\item \textbf{Corner Control} - Corners are very important in Reversi, so we will reward the player for controlling conrners, and similarly punish them for giving up corners to the opponent
\item \textbf{Corner Closeness} - In contrast with common intuition to the previous factor, being close to a corner will allow the opponent to get many coins in a single move, so we will reward the player when they force the opponent into this state
\item \textbf{Mobility} - SImilar to coin parity, only this time we look at the number of possible moves each player has \emph{from the current state} (as if they were playing)
\end{itemize}

Finally, a weighted average will be computed.

\subsection*{2. Heuristic Motivation}
Explained in the previous segment

\subsection*{4. Simple Player vs. Better Player}
The better player won all 6 matches (since we tuned the weights...) \\~\\
Since both players are deterministic, there is no sense in running multiple occurances of the same setup...

\section*{D. Alpha-Beta Player}
\subsection*{3. Comparison with Min-Max player}
We expect the alpha-beta player to have the upper hand when planning complex moves, since it can acheive more depth (when pruning is available), and thus `outsmart" the other player. \\~\\
However, the following must be taken into account
\begin{itemize}
\item When the time per turn is large, there shouldn't be much difference between players (unless somehow the extra depths allows the alpha-beta player to steer towards a guaranteed victory)
\item When nearing the end of the game, the min-max player would have equal chances, and thus a tie is probable
\item Under optimal conditions where the strategy tree is sorted (and thus pruning is optimal) and the time per turn is very small, the min-max player should fail completely
\end{itemize}

\section*{E. Further Improvements}
\subsection*{1. Expected Best Player}
We expect the Alpha-Beta player to perform best (assuming all players use the same heuristic), since
\begin{itemize}
\item The better player is, by definition, better than the simple player
\item The better player is the same as the Mini-Max player when the search is constrained to a depth of $1$, so the Mini-Max player has the advantage
\item The alpha-beta player - worst case - searches to the same depth as the Mini-Max player, and assuming pruning (remember $O(b^{\frac{3}{4} d})$ in the average case), it should perform best
\end{itemize}

\subsection*{2. Additional Methods}
\subsubsection*{Selective Deepening}
This would allow us to improve our decision when facing noisy states in the maximal depth of the strategy tree (noisy, meaning that there is a substancial flactuation in values). Thus, our decision would become more informed. \\~\\
Our design would consider ... TODO

\subsubsection*{Time For Step}
This would allow us to distribute our runtime between steps, so that more important steps get more time, or vice versa simple steps are efficiently performed. \\~\\
Our design would consider ... TODO

\section*{F. Using a game Data Bank}
\subsection*{1. \emph{Logistello} analysis}
dsa

\subsection*{2. Data Bank}
\subsubsection*{I. Opening Book Explanation}
An opening book is a data-structure which is based on recorder games, in which the most popular openings are stored in order to improve the performance of our player 
\begin{itemize}
\item It saves us from performing a heuristic search at the start of the game, and thus allows us to get results at depths that the runtime constraint could not allow us (in certain cases, probably not in ours where the depth is $10$)	
\item Accesing the opening book should be in $O(1)$ time, thus allowing us to use our run time for different purposes (like performing a heuristic search even deeper than the book's depth allows us to consider)
\end{itemize}

\subsubsection*{II. 5 Most Popular Openings}
dsa

\subsubsection*{VI. Disadvantage of `Most Popular' Opening Book}
There are two disadvantages
\begin{itemize}
\item Our player is predictable, so a clever opponent could navigate us to a game state where they have the highest chance of winning
\item Were ranking openings by their popularity --- not their efficiency. Like \emph{Logistello} did, it would be smarter to perform a regression over the game data in order to learn the best openings.
\end{itemize}

\subsubsection*{V. Further Possibilities for Usage of the Data Bank}
dsa

\section*{G. Conclusions}
\subsection*{1 .Graph of Scores}
dsa

\subsection*{2. Table of Scores}
dsa

\subsection*{3. Analysis of Scores}
dsa

\end{document}