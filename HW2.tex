\documentclass{article}      
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage[table,xcdraw]{xcolor}

\begin{document}

\title{%
  236501: Assignment \#2 \\
  \large Reversi \\
    Fall Semester '17-'18}

\author{
  Avidan, Eyal \\
  \texttt{205796469}
  \and
  Goaz, Or \\
  \texttt{307950113}
}

\maketitle

\section*{A. Simple Player}
\subsection*{1. Simple Player vs. Random Player}
The simple player's heuristic just tries to maximize the number of units the player has over the opponent (unless a goal state / lose state is acheived), which is a \textbf{very simple} method. When running against the \emph{random} player, the simple player one 2 out of 3 matches - both as the starting player and not. \\~\\
This really indicates the fact that the simple heuristic is no a good strategy, as it's prone to lead to foolish moves an expert player would never make, and thus the random player was able to beat it (twice!)

\section*{B. Better Player}
\subsection*{1. Heuristic Explanation}
We will use a heuristic comprised of the following factors
\begin{itemize}
\item \textbf{Coin Parity} - Similar to what the Simple Player attempted, only a negative weight will be set for having less units than the opponent, and instead of looking at the difference we will look at the share of coins owned by the player, e.g.
$$
	sign(my > op) \cdot \frac {my}{my + op}
$$
\item \textbf{Corner Control} - Corners are very important in Reversi, so we will reward the player for controlling conrners, and similarly punish them for giving up corners to the opponent
\item \textbf{Corner Closeness} - In contrast with common intuition to the previous factor, being close to a corner will allow the opponent to get many coins in a single move, so we will reward the player when they force the opponent into this state
\item \textbf{Mobility} - Similar to coin parity, only this time we look at the number of possible moves each player has \emph{from the current state} (as if they were playing)
\item \textbf{Stability} - On the board, each square has a value for the game, and this value depends on square location on the board. The more this value is greater, the more this square is important for the game. Summarize all the values of the player's square, give the stability of the player against his opponent.
\item \textbf{Frontier Discs} - Discs that are close by at list one empty square, are more volatile because the chance of being flipped is greater. So, we want to minimize this number of discs in order to keep more discs.
\end{itemize}


Finally, a weighted average will be computed. Each parameter get a weight according to it's importance.

\subsection*{2. Heuristic Motivation}
Explained in the previous segment

\subsection*{4. Simple Player vs. Better Player}
The better player won all 6 matches .
The better player, has more sophisticated heuristic. This heuristic consider the number of discs, but also calculate extra parameters and pay attention especially for corners, that are very important in Reversi.
Since both players are deterministic, there is no sense in running multiple occurances of the same setup.

\section*{D. Alpha-Beta Player}
\subsection*{3. Comparison with Min-Max player}
We expect the alpha-beta player to have the upper hand when planning complex moves, since it can acheive more depth (when pruning is available), and thus `outsmart" the other player. \\~\\
However, the following must be taken into account
\begin{itemize}
\item When the time per turn is large, there shouldn't be much difference between players (unless somehow the extra depths allows the alpha-beta player to steer towards a guaranteed victory)
\item When nearing the end of the game, the min-max player would have equal chances, and thus a tie is probable
\item Under optimal conditions where the strategy tree is sorted (and thus pruning is optimal) and the time per turn is very small, the min-max player should fail completely
\end{itemize}

\section*{E. Further Improvements}
\subsection*{1. Expected Best Player}
We expect the Alpha-Beta player to perform best (assuming all players use the same heuristic), since
\begin{itemize}
\item The better player is, by definition, better than the simple player
\item The better player is the same as the Mini-Max player when the search is constrained to a depth of $1$, so the Mini-Max player has the advantage
\item The alpha-beta player - worst case - searches to the same depth as the Mini-Max player, and assuming pruning (remember $O(b^{\frac{3}{4} d})$ in the average case), it should perform best
\end{itemize}

\subsection*{2. Additional Methods}
\subsubsection*{Selective Deepening}
This would allow us to improve our decision when facing noisy states in the maximal depth of the strategy tree (noisy, meaning that there is a substancial flactuation in values). Thus, our decision would become more informed. \\
In addition, Selective Deepening  prevent wasting time on noisy states and consume the time more efficiently. Thus, the algorithm can  Deepen more than before and thus get more informed decision.
\\~\\
Our design would consider the heuristic value of each state, depend on limited depth search. Each step of deepening, we remove states that their heuristic value is under defined threshold, or choose fixed number of top states to continue with.

\subsubsection*{Time For Step}
This would allow us to distribute our runtime between steps, so that more important steps get more time, or vice versa simple steps are efficiently performed. \\~\\
Our design would consider in the player state each step. We can compare the player's heuristic against the opponent's heuristic and produce a value that measure how "bad" the state of the player.\\
This evaluation should be calculated before the search step, provide more time cosider to "how bad the player's state" or continue until the player finds a way to get out from this "bad condition".

\section*{F. Using a game Data Bank}
\subsection*{1. \emph{Logistello} analysis}
\textbf{Logistello} uses a few evaluation methods to win
\begin{itemize}
\item Although the heuristic is also a linear combination of features, both features and their weights differ in 13 different game stages
\item The features are based upon patterns (diagonals, rows, columns, edges, corners), and a simple parity calculation
\item At the early stages of the game, it uses an opening book
\item It uses the Prob-Cut algorithm which prunes more aggresively than $\alpha\beta$
\item It doesn't play deterministically, and copies the opponent's winning moves --- so as not to lose twice in the same way
\end{itemize}

\subsection*{2. Data Bank}
\subsubsection*{I. Opening Book Explanation}
An opening book is a data-structure which is based on recorder games, in which the most popular openings are stored in order to improve the performance of our player 
\begin{itemize}
\item It saves us from performing a heuristic search at the start of the game, and thus allows us to get results at depths that the runtime constraint could not allow us (in certain cases, probably not in ours where the depth is $10$)	
\item Accesing the opening book should be in $O(1)$ time, thus allowing us to use our run time for different purposes (like performing a heuristic search even deeper than the book's depth allows us to consider)
\end{itemize}

\subsubsection*{II. 5 Most Popular Openings}
We add the function \emph{ExtractMostPopularOpenningMoves} to \emph{utils.py}.
The fifth most popular openning moves are:(before mirroring to adapt the data to our game)\\
1 = '+d3-c5+e6-f5+f6-e3+c3-f3+c4-b4' : 13493\\
2 = '+d3-c5+f6-e3+c3-f5+e6-f3+c4-b4' : 13493\\
3 = '+d3-c5+f6-f5+e6-e3+c3-f3+c4-b4' : 13493\\
4 =  '+d3-c5+e6-f5+f6-e3+d6-f7+g6-e7' : 7432\\
5 = '+d3-c5+f6-f5+e6-e3+d6-f7+g6-e7' : 7432\\

\subsubsection*{VI. Disadvantage of `Most Popular' Opening Book}
There are two disadvantages
\begin{itemize}
\item Our player is predictable, so a clever opponent could navigate us to a game state where they have the highest chance of winning
\item Were ranking openings by their popularity --- not their efficiency. Like \emph{Logistello} did, it would be smarter to perform a regression over the game data in order to learn the best openings.
\end{itemize}

\subsubsection*{V. Further Possibilities for Usage of the Data Bank}
The data bank can be using for:
\begin{itemize}
\item We can create top opening moves in a different way. The ranking will use also the game outcome.\\
Our design consider calculation of new score for the opening moves-
$$frequency \cdot \frac {winning - losses}{winning+ losses} $$
Opening moves of loosing games will not be used at all, and opening moves of winning games will get higher score according to it's frequency.
\item The book is usefull to learn techniques of players, and we can add parameter to our heuristic, that give every state a possibility for winning (by observing the winning vs the losses). By finding a state that has more winnings, our  heuristic will become more informed.\\
Our design consider the calculation:
$$\frac {100 \cdot Frequency}{Total} \cdot \frac {winning - losses}{winning+ losses} $$
This way, we punish our state according to how bad is our state observing old games and reward the player for being in a good state that have more possibility to win. We should consider the weight of this parameter. If the bank is dynamic, also the weight should be dynamic.
\end{itemize}

\section*{G. Conclusions}
\subsection*{1 .Graph of Scores}
\includegraphics{experiments}
\subsection*{2. Table of Scores}
\begin{table}[htp]
\centering
\caption{Player's Scores}
\label{my-label}
\begin{tabular}{@{}|
>{\columncolor[HTML]{C0C0C0}}l |lll@{}}
\toprule
\textbf{player}              & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\textbf{2}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\textbf{10}} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}\textbf{50}} \\ \midrule
\textbf{alpha\_beta\_player} & 16.0                                                    & 6.5                                                      & 10.0                                                     \\ \cmidrule(r){1-1}
\textbf{better\_player}      & 20.0                                                    & 21.0                                                     & 16.5                                                     \\ \cmidrule(r){1-1}
\textbf{min\_max\_player}    & 15.0                                                    & 23.0                                                     & 25.5                                                     \\ \cmidrule(r){1-1}
\textbf{simple\_player}      & 9.0                                                     & 9.5                                                      & 8.0                                                      \\ \bottomrule
\end{tabular}
\end{table}

\subsection*{3. Analysis of Scores}
Simple versus Better:
\begin{itemize}
\item Better wins Simple everytime. Better is more sophisticated heuristic, that consider the Coin Parity parameter (similar to simple) but add more features to make the heuristic more informed.
\item This is not suprising since we compre the players in exercise B.
\end{itemize}
Simple versus Minimax:
\begin{itemize}
\item Minimax wins simple most of the time. Minimax use the same heuristic as Better, so it is not suprising it overpower Simple.
\item Because Minimax checks always all the brances of the tree, it suppose not to loose since it use Better heuristic. Those few losses may indicate that there is a problem in the weights of the parameters in the heuristic. 
\end{itemize}
Simple versus AlphaBeta:
\begin{itemize}
\item This result is suprising. First, AlphaBeta wins Simple about half of the games, and Simple wins AlphaBeta about half of the games.
\item We assume, that AlphaBeta use the heuristic very good until specific depth. When he has time, he gets deeper and deeper and it may change the whole value of the score. That can be cause of bad scoring in specific state of the game.(Depends on the weights of every parameter in calculation)
\item Another assumption is that AlphaBeta prune brances that are possible to be good deeper in the tree, and that may cause the algorithm miss good strategies.
\end{itemize}
Better versus Minimax:
\begin{itemize}
\item Minimax wins Better most of the times. This results were predicted, because Better is like Minimax, but with limitation to 1 deepening. Since MiniMax can get more deeper, it is more informed and can see further than Better.
\item We can notice, that on 2 seconds games, MiniMax is almost equal to Better. We assume that MiniMax doesn't have enough time   to get deeper.
\end{itemize}
Better versus AlphaBeta:
\begin{itemize}
\item Another suprsing result. It's looks like Better has more winnings than AlphaBeta in theirs games.
\item The reasons, may be similar to thos we mentioned earlier on Simple versus AlphaBeta examination.
\end{itemize}
\newpage
Minimax versus AlphaBeta:
\begin{itemize}
\item Again, surprisingly, MiniMax is much better than AlphaBeta. On 2 seconds games they are almost equal, but on 10 and 50 seconds games MiniMax has much more winnings. 
\item The reason, is again, one of one of the previoud assumption. Those results, clarify that it's more likely that the weights of the heuristic's parameters in a specific states are not good.Those bad weights may cause mistakes on prunning and bad decisions.
\end{itemize}

In conculsion, the graph different from what we expected. We expected to see AlphaBeta wins most of the time, and we less winnings for Simple.\\
In regular competition, Better's hueristic wins Simple's heuristic, but when we use more complex algorithm like MiniMax and AlphaBeta suprising results can be occured.\\
An extreme point, can be seen on MiniMax line on 50 seconds. This extreme point is very rational, because MiniMax have more time to get deeper in the tree and make a better decesion.AlphaBeta rising on 50 seconds indicates that there is a problem with the weights in specific depth of the tree, that make this algorithm loose(Mostly cause of bad prunning).
\end{document}